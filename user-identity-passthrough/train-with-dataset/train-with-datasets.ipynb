{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/MachineLearningNotebooks/how-to-use-azureml/work-with-data/datasets-tutorial/train-with-datasets/train-with-datasets.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train with Azure Machine Learning datasets\n",
    "Datasets are categorized into TabularDataset and FileDataset based on how users consume them in training. \n",
    "* A TabularDataset represents data in a tabular format by parsing the provided file or list of files. TabularDataset can be created from csv, tsv, parquet files, SQL query results etc. For the complete list, please visit our [documentation](https://aka.ms/tabulardataset-api-reference). It provides you with the ability to materialize the data into a pandas DataFrame.\n",
    "* A FileDataset references single or multiple files in your datastores or public urls. This provides you with the ability to download or mount the files to your compute. The files can be of any format, which enables a wider range of machine learning scenarios including deep learning.\n",
    "\n",
    "In this tutorial, you will learn how to train with Azure Machine Learning datasets:\n",
    "\n",
    "&#x2611; Use datasets directly in your training script\n",
    "\n",
    "&#x2611; Use datasets to mount files to a remote compute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please install the private build using the following pip command on terminal:\n",
    "- pip install azureml-sdk==0.1.0.* --index-url https://azuremlsdktestpypi.azureedge.net/DataDrift-SDK-Unit/25355784  --extra-index-url https://pypi.python.org/simple\n",
    "- pip install azureml-dataprep[pandas]\n",
    "- pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK version: 0.1.0.25355784\n"
     ]
    }
   ],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "\n",
    "print('SDK version:', azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Workspace\n",
    "\n",
    "Initialize a workspace object from persisted configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Falling back to use azure cli login credentials.\n",
      "If you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\n",
      "Please refer to aka.ms/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rafarmahcredpassthrucanary\n",
      "rafarmahtestrg\n",
      "eastus2euap\n",
      "b1fff005-d722-4d97-99ac-7c6e9ef020aa\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "# please update this with your workspace information. Make sure your workspace is whitelisted\n",
    "subscriptionid='b1fff005-d722-4d97-99ac-7c6e9ef020aa'\n",
    "resourcegroup='rafarmahtestrg'\n",
    "workspacename='rafarmahcredpassthrucanary'\n",
    "\n",
    "#subscriptionid={YourSubscriptionID}\n",
    "#resourcegroup={YourResourceGroup}\n",
    "#workspacename={WorkspaceName}\n",
    "\n",
    "ws = Workspace(subscriptionid, resourcegroup, workspacename)\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Experiment\n",
    "\n",
    "**Experiment** is a logical container in an Azure ML Workspace. It hosts run records which can include run metrics and output artifacts from your experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "experiment_name = 'credential-passthrough'\n",
    "experiment = Experiment(workspace = ws, name = experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create or Attach existing compute resource\n",
    "By using Azure Machine Learning Compute, a managed service, data scientists can train machine learning models on clusters of Azure virtual machines. Examples include VMs with GPU support. In this tutorial, you create Azure Machine Learning Compute as your training environment. The code below creates the compute clusters for you if they don't already exist in your workspace.\n",
    "\n",
    "**Creation of compute takes approximately 5 minutes.** If the AmlCompute with that name is already in your workspace the code will skip the creation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing cluster, use it.\n",
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "\n",
      "Minimum number of nodes requested have been provisioned\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# Choose a name for your CPU cluster. We've whitelisted the passthrucluster1,2,3. \n",
    "# If you want to whitelist a specific name for your cluster, please contact sihhu@microsoft.com\n",
    "cpu_cluster_name = 'passthrucluster1' # \"passthrucluster2\", \"passthrucluster3\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    cpu_cluster = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2',\n",
    "                                                           max_nodes=4)\n",
    "    cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n",
    "\n",
    "cpu_cluster.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have the necessary packages and compute resources to train a model in the cloud.\n",
    "## Use datasets directly in training\n",
    "\n",
    "### Create a TabularDataset\n",
    "By creating a dataset, you create a reference to the data source location. If you applied any subsetting transformations to the dataset, they will be stored in the dataset as well. The data remains in its existing location, so no extra storage cost is incurred. \n",
    "\n",
    "First you need to upload [iris dataset](./train-dataset/iris.csv) to your ADLS Gen2 storage account. Make sure that you grant yourself 'Storage Blob Data Contributor' access to the storage account for read & write access. ADLS Gen 2 also supports POSIX-like access control lists (ACLs), learn how to set ACLs [here](https://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-access-control)\n",
    "\n",
    "![roleaccess](roleaccess.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will create an unregistered TabularDataset pointing from ADLS Gen2 storage url. You can also create a dataset from multiple paths. [learn more](https://aka.ms/azureml/howto/createdatasets) <br>\n",
    "You can find the storage url from storage explorer on Azure portal\n",
    "![image.png](storageurl.jpg)\n",
    "\n",
    "[TabularDataset](https://docs.microsoft.com/python/api/azureml-core/azureml.data.tabulardataset?view=azure-ml-py) represents data in a tabular format by parsing the provided file or list of files. This provides you with the ability to materialize the data into a Pandas or Spark DataFrame. You can create a TabularDataset object from .csv, .tsv, and parquet files, and from SQL query results. For a complete list, see [TabularDatasetFactory](https://docs.microsoft.com/python/api/azureml-core/azureml.data.dataset_factory.tabulardatasetfactory?view=azure-ml-py) class.\n",
    "\n",
    "**NOTE** You will get permission denied if you try to load data from the sample url below because you do not have permission to the adlsgen2 storage account. You need to upload [iris dataset](./train-dataset/iris.csv) to your adlsgen2 storage account and replace the url with your own storage url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "dataset-remarks-tabular-sample"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Credentials are not provided to access data from the source. Please sign in using identity with required permission granted.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width      species\n",
       "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
       "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
       "2           4.7          3.2           1.3          0.2  Iris-setosa"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.core import Dataset\n",
    "dataset = Dataset.Tabular.from_delimited_files('https://mayadls2.blob.core.windows.net/tabular/iris.csv')\n",
    "\n",
    "# preview the first 3 rows of the dataset\n",
    "dataset.take(3).to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can create ADLS Gen2 without providing credentials and create dataset from datastore path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width      species\n",
       "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
       "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
       "2           4.7          3.2           1.3          0.2  Iris-setosa"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.core import Datastore\n",
    "datastore = Datastore.register_azure_data_lake_gen2(workspace=ws, datastore_name='mayadlsgen2',\n",
    "                                                   filesystem='tabular', account_name='mayadls2')\n",
    "dataset = Dataset.Tabular.from_delimited_files((datastore, 'iris.csv'))\n",
    "dataset.take(3).to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a training script\n",
    "\n",
    "To submit the job to the cluster, first create a training script. Run the following code to create the training script called `train_titanic.py` in the script_folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "script_folder = os.path.join(os.getcwd(), 'train-dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting C:\\Users\\SIHHU\\project\\identity-based-data-access\\user-identity-passthrough\\train-with-dataset\\train-dataset/train_iris.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $script_folder/train_iris.py\n",
    "\n",
    "import os\n",
    "\n",
    "from azureml.dataprep import __version__ as dprepver\n",
    "from azureml.core import Dataset, Run\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# sklearn.externals.joblib is removed in 0.23\n",
    "from sklearn import __version__ as sklearnver\n",
    "from packaging.version import Version\n",
    "if Version(sklearnver) < Version(\"0.23.0\"):\n",
    "    from sklearn.externals import joblib\n",
    "else:\n",
    "    import joblib\n",
    "\n",
    "print('dprep version: {}'.format(dprepver))\n",
    "\n",
    "run = Run.get_context()\n",
    "# get input dataset by name\n",
    "dataset = run.input_datasets['iris']\n",
    "\n",
    "df = dataset.to_pandas_dataframe()\n",
    "\n",
    "x_col = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
    "y_col = ['species']\n",
    "x_df = df.loc[:, x_col]\n",
    "y_df = df.loc[:, y_col]\n",
    "\n",
    "#dividing X,y into train and test data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_df, y_df, test_size=0.2, random_state=223)\n",
    "\n",
    "data = {'train': {'X': x_train, 'y': y_train},\n",
    "\n",
    "        'test': {'X': x_test, 'y': y_test}}\n",
    "\n",
    "clf = DecisionTreeClassifier().fit(data['train']['X'], data['train']['y'])\n",
    "model_file_name = 'decision_tree.pkl'\n",
    "\n",
    "print('Accuracy of Decision Tree classifier on training set: {:.2f}'.format(clf.score(x_train, y_train)))\n",
    "print('Accuracy of Decision Tree classifier on test set: {:.2f}'.format(clf.score(x_test, y_test)))\n",
    "\n",
    "os.makedirs('./outputs', exist_ok=True)\n",
    "with open(model_file_name, 'wb') as file:\n",
    "    joblib.dump(value=clf, filename='outputs/' + model_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an environment\n",
    "\n",
    "Define a conda environment YAML file with your training script dependencies and create an Azure ML environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting conda_dependencies.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile conda_dependencies.yml\n",
    "\n",
    "dependencies:\n",
    "- python=3.6.2\n",
    "- scikit-learn\n",
    "- pip=20.2.4\n",
    "- pip:\n",
    "  - packaging\n",
    "  - nltk\n",
    "  - azureml-defaults\n",
    "  - azureml-telemetry\n",
    "  - azureml-dataprep[pandas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "\n",
    "sklearn_env = Environment.from_conda_specification(name = 'sklearn-env', file_path = './conda_dependencies.yml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure training run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ScriptRunConfig object specifies the configuration details of your training job, including your training script, environment to use, and the compute target to run on. Specify the following in your script run configuration:\n",
    "* The directory that contains your scripts. All the files in this directory are uploaded into the cluster nodes for execution. \n",
    "* The training script name, train_iris.py\n",
    "* The input dataset for training, passed as an argument to your training script. `as_named_input()` is required so that the input dataset can be referenced by the assigned name in your training script. \n",
    "* The compute target. In this case you will use the AmlCompute you created\n",
    "* The environment definition for the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import ScriptRunConfig\n",
    "\n",
    "src = ScriptRunConfig(source_directory=script_folder,\n",
    "                      script='train_iris.py',\n",
    "                      arguments=[dataset.as_named_input('iris')],\n",
    "                      compute_target=cpu_cluster,\n",
    "                      environment=sklearn_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit job to run\n",
    "Submit the ScriptRunConfig to the Azure ML experiment to kick off the execution. You will need to set `credential_passthrough=True` to opt-in to use your own identity for data access authentication in remote training. Otherwise, our service will try to use the identity of the compute for data access authentication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>credential-passthrough</td><td>credential-passthrough_1612229920_361138c2</td><td>azureml.scriptrun</td><td>Preparing</td><td><a href=\"https://ml.azure.com/experiments/credential-passthrough/runs/credential-passthrough_1612229920_361138c2?wsid=/subscriptions/b1fff005-d722-4d97-99ac-7c6e9ef020aa/resourcegroups/rafarmahtestrg/workspaces/rafarmahcredpassthrucanary\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.script_run.ScriptRun?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
      ],
      "text/plain": [
       "Run(Experiment: credential-passthrough,\n",
       "Id: credential-passthrough_1612229920_361138c2,\n",
       "Type: azureml.scriptrun,\n",
       "Status: Preparing)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run = experiment.submit(src, credential_passthrough=True)\n",
    "run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use datasets to mount files to a remote compute\n",
    "\n",
    "You can use the `Dataset` object to mount or download files referred by it. When you mount a file system, you attach that file system to a directory (mount point) and make it available to the system. Because mounting load files at the time of processing, it is usually faster than download.<br> \n",
    "Note: mounting is only available for Linux-based compute (DSVM/VM, AMLCompute, HDInsights)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload data files into your ADLS Gen2 storage account\n",
    "We will first load diabetes data from `scikit-learn` to the train-dataset folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "import numpy as np\n",
    "\n",
    "os.makedirs('./data', exist_ok=True)\n",
    "training_data = load_diabetes()\n",
    "np.save(file='./data/features.npy', arr=training_data['data'])\n",
    "np.save(file='./data/labels.npy', arr=training_data['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now upload the 2 files into the ADLS Gen2 storage account into a folder named `diabetes`and create a FileDataset referencing it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Method upload_directory: This is an experimental method, and may change at any time.<br/>For more information, see https://aka.ms/azuremlexperimental.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating arguments.\n",
      "Arguments validated.\n",
      "Uploading file to diabetes\n",
      "Uploading an estimated of 2 files\n",
      "Target already exists. Skipping upload for diabetes\\features.npy\n",
      "Target already exists. Skipping upload for diabetes\\labels.npy\n",
      "Uploaded 0 files\n",
      "Creating new dataset\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Dataset\n",
    "\n",
    "dataset = Dataset.File.upload_directory(src_dir='./data',target=(datastore,'diabetes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a training script\n",
    "\n",
    "To submit the job to the cluster, first create a training script. Run the following code to create the training script called `train_diabetes.py` in the script_folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting C:\\Users\\SIHHU\\project\\identity-based-data-access\\user-identity-passthrough\\train-with-dataset\\train-dataset/train_diabetes.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $script_folder/train_diabetes.py\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import argparse\n",
    "\n",
    "from azureml.core.run import Run\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "# sklearn.externals.joblib is removed in 0.23\n",
    "from sklearn import __version__ as sklearnver\n",
    "from packaging.version import Version\n",
    "if Version(sklearnver) < Version(\"0.23.0\"):\n",
    "    from sklearn.externals import joblib\n",
    "else:\n",
    "    import joblib\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data-folder', type=str, help='training dataset')\n",
    "args = parser.parse_args()\n",
    "\n",
    "os.makedirs('./outputs', exist_ok=True)\n",
    "\n",
    "base_path = args.data_folder\n",
    "\n",
    "run = Run.get_context()\n",
    "\n",
    "X = np.load(glob.glob(os.path.join(base_path, '**/features.npy'), recursive=True)[0])\n",
    "y = np.load(glob.glob(os.path.join(base_path, '**/labels.npy'), recursive=True)[0])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=0)\n",
    "data = {'train': {'X': X_train, 'y': y_train},\n",
    "        'test': {'X': X_test, 'y': y_test}}\n",
    "\n",
    "# list of numbers from 0.0 to 1.0 with a 0.05 interval\n",
    "alphas = np.arange(0.0, 1.0, 0.05)\n",
    "\n",
    "for alpha in alphas:\n",
    "    # use Ridge algorithm to create a regression model\n",
    "    reg = Ridge(alpha=alpha)\n",
    "    reg.fit(data['train']['X'], data['train']['y'])\n",
    "\n",
    "    preds = reg.predict(data['test']['X'])\n",
    "    mse = mean_squared_error(preds, data['test']['y'])\n",
    "    run.log('alpha', alpha)\n",
    "    run.log('mse', mse)\n",
    "\n",
    "    model_file_name = 'ridge_{0:.2f}.pkl'.format(alpha)\n",
    "    with open(model_file_name, 'wb') as file:\n",
    "        joblib.dump(value=reg, filename='outputs/' + model_file_name)\n",
    "\n",
    "    print('alpha is {0:.2f}, and mse is {1:0.2f}'.format(alpha, mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure & Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now configure your run. We will reuse the same `sklearn_env` environment from the previous run. Once the environment is built, and if you don't change your dependencies, it will be reused in subsequent runs. \n",
    "\n",
    "We will pass in the DatasetConsumptionConfig of our FileDataset to the `'--data-folder'` argument of the script. Azure ML will resolve this to mount point of the data on the compute target, which we parse in the training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import ScriptRunConfig\n",
    "\n",
    "src = ScriptRunConfig(source_directory=script_folder, \n",
    "                      script='train_diabetes.py', \n",
    "                      # to mount the dataset on the remote compute and pass the mounted path as an argument to the training script\n",
    "                      arguments =['--data-folder', dataset.as_mount()],\n",
    "                      compute_target=cpu_cluster,\n",
    "                      environment=sklearn_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to set `credential_passthrough=True` to opt-in to use your own identity for data access authentication in remote training. Otherwise, our service will try to use the identity of the compute for data access authentication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = experiment.submit(config=src, credential_passthrough=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display run results\n",
    "You now have a model trained on a remote cluster. Retrieve all the metrics logged during the run, including the accuracy of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': [0.0, 0.05, 0.1, 0.15000000000000002, 0.2, 0.25, 0.30000000000000004, 0.35000000000000003, 0.4, 0.45, 0.5, 0.55, 0.6000000000000001, 0.65, 0.7000000000000001, 0.75, 0.8, 0.8500000000000001, 0.9, 0.9500000000000001], 'mse': [3424.3166882137343, 3408.9153122589296, 3372.649627810032, 3345.14964347419, 3325.294679467878, 3311.5562509289744, 3302.6736334017264, 3297.658733944204, 3295.74106435581, 3296.316884705676, 3298.9096058070622, 3303.140055527517, 3308.7042707723226, 3315.3568399622573, 3322.898314903962, 3331.1656169285875, 3340.024662032161, 3349.364644348603, 3359.093569748443, 3369.1347399130477]}\n"
     ]
    }
   ],
   "source": [
    "run.wait_for_completion()\n",
    "metrics = run.get_metrics()\n",
    "print(metrics)"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "sihhu"
   }
  ],
  "category": "tutorial",
  "compute": [
   "Remote"
  ],
  "datasets": [
   "Iris",
   "Diabetes"
  ],
  "deployment": [
   "None"
  ],
  "exclude_from_index": false,
  "framework": [
   "Azure ML"
  ],
  "friendly_name": "Train with Datasets (Tabular and File)",
  "index_order": 1,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "star_tag": [
   "featured"
  ],
  "tags": [
   "Dataset",
   "Estimator",
   "ScriptRun"
  ],
  "task": "Train"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
